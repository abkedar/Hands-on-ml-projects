{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Predictive_Modeling_for_Sentiment_Analysis.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Oy_cnwscRoD"
      },
      "source": [
        "# Import necessary dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "nHKycl44cRoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "680d6fe5-b412-4c5e-8236-0877d329f510"
      },
      "source": [
        "# Import required Libraries\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from contractions import CONTRACTION_MAP\n",
        "import unicodedata\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nlp = spacy.load('en', parse = False, tag=False, entity=False)\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "stopword_list.remove('no')\n",
        "stopword_list.remove('not')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS9IDAV7k-wP",
        "outputId": "1a8d6bc5-73cb-462b-868d-f12fef8318a1"
      },
      "source": [
        "# Load train Data set\r\n",
        "df = pd.read_csv(\"/content/sentiment.csv\", error_bad_lines=False, sep='\\t')\r\n",
        "#df.head()\r\n",
        "\r\n",
        "df.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHh-mv72zsd_"
      },
      "source": [
        "# load test data\r\n",
        "df_ts = pd.read_csv(\"/content/sentiment_tst.csv\", error_bad_lines=False, sep='\\t')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reuLBbg2cRoP"
      },
      "source": [
        "# Cleaning Text - strip HTML"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ExoBRd4lcRoP"
      },
      "source": [
        "# function to remove html code in the text rwas data\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")#.get_text()\n",
        "    stripped_text = soup.get_text()\n",
        "    return stripped_text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqsBqWRlcRoQ"
      },
      "source": [
        "# Removing accented characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "5_s-ujGLcRoQ"
      },
      "source": [
        "# Function bring the text to normal string format\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WhogiJFcRoQ"
      },
      "source": [
        "# Expanding Contractions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dsp9mUqTcRoR"
      },
      "source": [
        "# Function text data contain word like don't, does'nt, so convert them to do not, does not\n",
        "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
        "    \n",
        "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
        "                                      flags=re.IGNORECASE|re.DOTALL)\n",
        "    def expand_match(contraction):\n",
        "        match = contraction.group(0)\n",
        "        first_char = match[0]\n",
        "        expanded_contraction = contraction_mapping.get(match)\\\n",
        "                                if contraction_mapping.get(match)\\\n",
        "                                else contraction_mapping.get(match.lower())                       \n",
        "        expanded_contraction = first_char+expanded_contraction[1:]\n",
        "        return expanded_contraction\n",
        "        \n",
        "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
        "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
        "    return expanded_text"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWNNXNjGcRoR"
      },
      "source": [
        "# Removing Special Characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "FRb_hlwwcRoS"
      },
      "source": [
        "# Function remove special character other then alphabet and number\n",
        "def remove_special_characters(text):\n",
        "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
        "    return text"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmdhORKLcRoS"
      },
      "source": [
        "# Lemmatizing text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Kb0p_U7_cRoT"
      },
      "source": [
        "# Function bring the pural, abjective word to root form.\n",
        "def lemmatize_text(text):\n",
        "    text = nlp(text)\n",
        "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
        "    return text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggTrklg7cRoT"
      },
      "source": [
        "# Removing Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "PUSv7375cRoT"
      },
      "source": [
        "# Function to remove stopword using NLTK Libraries\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z67xaNFKcRoU"
      },
      "source": [
        "# Normalize text corpus - tying it all together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "9eUcYO2qcRoU"
      },
      "source": [
        "# Combining all above function in above function in one and carry the Text cleaning data\n",
        "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
        "                     accented_char_removal=True, text_lower_case=True, \n",
        "                     text_lemmatization=True, special_char_removal=True, \n",
        "                     stopword_removal=True):\n",
        "    \n",
        "    normalized_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # strip HTML\n",
        "        if html_stripping:\n",
        "            doc = strip_html_tags(doc)\n",
        "        # remove accented characters\n",
        "        if accented_char_removal:\n",
        "            doc = remove_accented_chars(doc)\n",
        "        # expand contractions    \n",
        "        if contraction_expansion:\n",
        "            doc = expand_contractions(doc)\n",
        "        # lowercase the text    \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        # remove extra newlines\n",
        "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "        # insert spaces between special characters to isolate them    \n",
        "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "        # lemmatize text\n",
        "        if text_lemmatization:\n",
        "            doc = lemmatize_text(doc)\n",
        "        # remove special characters    \n",
        "        if special_char_removal:\n",
        "            doc = remove_special_characters(doc)  \n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        # remove stopwords\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
        "            \n",
        "        normalized_corpus.append(doc)\n",
        "        \n",
        "    return normalized_corpus\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WRc5tzucRoV"
      },
      "source": [
        "# Model predictions of movie review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUpdAzJl4YUV"
      },
      "source": [
        "# Import Scikit Learn Libraries for model prediction.\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
        "\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.base import clone\r\n",
        "from sklearn.metrics import roc_curve, auc \r\n",
        "\r\n",
        "from sklearn import metrics"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldOjGeJEcRoX"
      },
      "source": [
        "# Run the Cleaning process function of text data in train data set\r\n",
        "df['cleaned_re'] = normalize_corpus(df['review'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5es_0Fvs9Mt"
      },
      "source": [
        "# Run the Cleaning process function of text data in test data set\r\n",
        "df_ts['cleaned_re'] = normalize_corpus(df_ts['review'])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "WgA3q_Lk55eU",
        "outputId": "b63d8deb-b954-4df9-d1fe-a7a3796e2730"
      },
      "source": [
        "df_ts.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>cleaned_re</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>The Violent Men is a good western. Perhaps the...</td>\n",
              "      <td>positive</td>\n",
              "      <td>violent man good western perhaps story not ori...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>QUESTION: How does a film merit two different ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>question film merit two different title like l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>The title pretty much lets you know what you'r...</td>\n",
              "      <td>negative</td>\n",
              "      <td>title pretty much let know get grade c howler ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>This could have been the best game ever!! But ...</td>\n",
              "      <td>negative</td>\n",
              "      <td>could good game ever game maker screw 3 assass...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Oliver Stone is not one to shy away from a mov...</td>\n",
              "      <td>positive</td>\n",
              "      <td>oliver stone not one shy away movie theme matt...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                         cleaned_re\n",
              "0           0  ...  violent man good western perhaps story not ori...\n",
              "1           1  ...  question film merit two different title like l...\n",
              "2           2  ...  title pretty much let know get grade c howler ...\n",
              "3           3  ...  could good game ever game maker screw 3 assass...\n",
              "4           4  ...  oliver stone not one shy away movie theme matt...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C73caT8h5l1-"
      },
      "source": [
        "# take a peek at the data\r\n",
        "reviews = np.array(df['cleaned_re'])\r\n",
        "sentiments = np.array(df['sentiment'])\r\n",
        "\r\n",
        "reviews_ts = np.array(df_ts['cleaned_re'])\r\n",
        "sentiments_ts = np.array(df_ts['sentiment'])\r\n",
        "\r\n",
        "# build train and test datasets\r\n",
        "norm_train_reviews = reviews\r\n",
        "train_sentiments = sentiments\r\n",
        "norm_test_reviews = reviews_ts\r\n",
        "test_sentiments = sentiments_ts\r\n",
        "\r\n",
        "# normalize datasets\r\n",
        "#norm_train_reviews = tn.normalize_corpus(train_reviews)\r\n",
        "#norm_test_reviews = tn.normalize_corpus(test_reviews)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5yhUlg09Etm"
      },
      "source": [
        "# build BOW features on train reviews\r\n",
        "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0, ngram_range=(1,2))\r\n",
        "cv_train_features = cv.fit_transform(norm_train_reviews)\r\n",
        "\r\n",
        "# build TFIDF features on train reviews\r\n",
        "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2),\r\n",
        "                     sublinear_tf=True)\r\n",
        "tv_train_features = tv.fit_transform(norm_train_reviews)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9anuBLH4i05"
      },
      "source": [
        "# transform test reviews into features\r\n",
        "cv_test_features = cv.transform(norm_test_reviews)\r\n",
        "tv_test_features = tv.transform(norm_test_reviews)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3eoLt814KDX"
      },
      "source": [
        "def train_predict_model(classifier, \r\n",
        "                        train_features, train_labels, \r\n",
        "                        test_features, test_labels):\r\n",
        "    # build model    \r\n",
        "    classifier.fit(train_features, train_labels)\r\n",
        "    # predict using model\r\n",
        "    predictions = classifier.predict(test_features) \r\n",
        "    return predictions "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZMmcNtoAes4"
      },
      "source": [
        "# Run the metrics function to display the performance of predictive modeling \r\n",
        "def display_classification_report(true_labels, predicted_labels, classes=[1,0]):\r\n",
        "\r\n",
        "    report = metrics.classification_report(y_true=true_labels, \r\n",
        "                                           y_pred=predicted_labels, \r\n",
        "                                           labels=classes) \r\n",
        "    print(report)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF-OK5-TAlbf"
      },
      "source": [
        "# Run the metrics function to display the performance of predictive modeling\r\n",
        "def display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\r\n",
        "    \r\n",
        "    total_classes = len(classes)\r\n",
        "    level_labels = [total_classes*[0], list(range(total_classes))]\r\n",
        "\r\n",
        "    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \r\n",
        "                                  labels=classes)\r\n",
        "    cm_frame = pd.DataFrame(data=cm, \r\n",
        "                            columns=pd.MultiIndex(levels=[['Predicted:'], classes], \r\n",
        "                                                  codes=level_labels), \r\n",
        "                            index=pd.MultiIndex(levels=[['Actual:'], classes], \r\n",
        "                                                codes=level_labels)) \r\n",
        "    print(cm_frame) "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vj8CWz3-dMJ"
      },
      "source": [
        "# Run the metrics function to display the performance of predictive modeling \r\n",
        "def display_model_performance_metrics(true_labels, predicted_labels, classes=[1,0]):\r\n",
        "    print('Model Performance metrics:')\r\n",
        "    print('-'*30)\r\n",
        "    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\r\n",
        "    print('\\nModel Classification report:')\r\n",
        "    print('-'*30)\r\n",
        "    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, \r\n",
        "                                  classes=classes)\r\n",
        "    print('\\nPrediction Confusion Matrix:')\r\n",
        "    print('-'*30)\r\n",
        "    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \r\n",
        "                             classes=classes)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8G3ftCq-uno"
      },
      "source": [
        "# Run the metrics function to display the performance of predictive modeling \r\n",
        "def get_metrics(true_labels, predicted_labels):\r\n",
        "    \r\n",
        "    print('Accuracy:', np.round(\r\n",
        "                        metrics.accuracy_score(true_labels, \r\n",
        "                                               predicted_labels),\r\n",
        "                        4))\r\n",
        "    print('Precision:', np.round(\r\n",
        "                        metrics.precision_score(true_labels, \r\n",
        "                                               predicted_labels,\r\n",
        "                                               average='weighted'),\r\n",
        "                        4))\r\n",
        "    print('Recall:', np.round(\r\n",
        "                        metrics.recall_score(true_labels, \r\n",
        "                                               predicted_labels,\r\n",
        "                                               average='weighted'),\r\n",
        "                        4))\r\n",
        "    print('F1 Score:', np.round(\r\n",
        "                        metrics.f1_score(true_labels, \r\n",
        "                                               predicted_labels,\r\n",
        "                                               average='weighted'),\r\n",
        "                        4))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwL5hnaW9bV1",
        "outputId": "a656a8c4-62e0-4ddb-a22b-946f45987ba9"
      },
      "source": [
        "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\r\n",
        "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOW model:> Train features shape: (25000, 1627917)  Test features shape: (25000, 1627917)\n",
            "TFIDF model:> Train features shape: (25000, 1627917)  Test features shape: (25000, 1627917)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJySmWUA9qQq"
      },
      "source": [
        "### Model Training, Prediction and Performance Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w26OcM3p9eIc"
      },
      "source": [
        "# Predictive modeling\r\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\r\n",
        "\r\n",
        "lr = LogisticRegression(penalty='l2', max_iter=100, C=1)\r\n",
        "svm = SGDClassifier(loss='hinge', max_iter=100)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waOV1F3Z9wVk",
        "outputId": "0508a0c4-3e85-418f-f9d4-6fd795f982a4"
      },
      "source": [
        "# Logistic Regression model on BOW features\r\n",
        "# Please Note : the module meu is not been provided. \r\n",
        "lr_bow_predictions = train_predict_model(classifier=lr, \r\n",
        "                                             train_features=cv_train_features, train_labels=train_sentiments,\r\n",
        "                                             test_features=cv_test_features, test_labels=test_sentiments)\r\n",
        "display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_bow_predictions,\r\n",
        "                                      classes=['positive', 'negative'])\r\n",
        "\r\n",
        "# THE BELOW O/P SHOULD GIVE YOU A FAIR IDEA ON WHAT :\r\n",
        "# methods like \r\n",
        "# train_predict_model() are doing and printing as o/p.\r\n",
        "# display_model_performance_metrics() are doing and printing as o/p.\r\n",
        "\r\n",
        "# As an Intern you are not suppose to produce the exact o/p \r\n",
        "# You may only code the minimum required metrics which helps you to \r\n",
        "# compare the different ML models."
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model Performance metrics:\n",
            "------------------------------\n",
            "Accuracy: 0.8887\n",
            "Precision: 0.8887\n",
            "Recall: 0.8887\n",
            "F1 Score: 0.8887\n",
            "\n",
            "Model Classification report:\n",
            "------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.89      0.89      0.89     12500\n",
            "    negative       0.89      0.89      0.89     12500\n",
            "\n",
            "    accuracy                           0.89     25000\n",
            "   macro avg       0.89      0.89      0.89     25000\n",
            "weighted avg       0.89      0.89      0.89     25000\n",
            "\n",
            "\n",
            "Prediction Confusion Matrix:\n",
            "------------------------------\n",
            "                 Predicted:         \n",
            "                   positive negative\n",
            "Actual: positive      11109     1391\n",
            "        negative       1391    11109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mJ28ixe_5h0",
        "outputId": "ba72ccd1-a14d-4659-aa41-f0f1b697bcb8"
      },
      "source": [
        "# Logistic Regression model on TF-IDF features\r\n",
        "# Please Note : the module meu is not been provided.\r\n",
        "lr_tfidf_predictions = train_predict_model(classifier=lr, \r\n",
        "                                               train_features=tv_train_features, train_labels=train_sentiments,\r\n",
        "                                               test_features=tv_test_features, test_labels=test_sentiments)\r\n",
        "display_model_performance_metrics(true_labels=test_sentiments, predicted_labels=lr_tfidf_predictions,\r\n",
        "                                      classes=['positive', 'negative'])\r\n",
        "\r\n",
        "# THE BELOW O/P SHOULD GIVE YOU A FAIR IDEA ON WHAT :\r\n",
        "# methods like \r\n",
        "# train_predict_model() are doing and printing as o/p.\r\n",
        "# display_model_performance_metrics() are doing and printing as o/p.\r\n",
        "\r\n",
        "# As an Intern you are not suppose to produce the exact o/p \r\n",
        "# You may only code the minimum required metrics which helps you to \r\n",
        "# compare the different ML models."
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Performance metrics:\n",
            "------------------------------\n",
            "Accuracy: 0.8856\n",
            "Precision: 0.8857\n",
            "Recall: 0.8856\n",
            "F1 Score: 0.8856\n",
            "\n",
            "Model Classification report:\n",
            "------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.88      0.89      0.89     12500\n",
            "    negative       0.89      0.88      0.89     12500\n",
            "\n",
            "    accuracy                           0.89     25000\n",
            "   macro avg       0.89      0.89      0.89     25000\n",
            "weighted avg       0.89      0.89      0.89     25000\n",
            "\n",
            "\n",
            "Prediction Confusion Matrix:\n",
            "------------------------------\n",
            "                 Predicted:         \n",
            "                   positive negative\n",
            "Actual: positive      11118     1382\n",
            "        negative       1477    11023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-feHIRIxCIIT",
        "outputId": "da6d1ae3-4120-4ee9-e020-dc12dacf75e3"
      },
      "source": [
        "# SVM model on BOW features\r\n",
        "# Please Note : the module meu is not been provided.\r\n",
        "svm_bow_predictions = train_predict_model(classifier=svm, \r\n",
        "                                             train_features=cv_train_features, train_labels=train_sentiments,\r\n",
        "                                             test_features=cv_test_features, test_labels=test_sentiments)\r\n",
        "display_model_performance_metrics(true_labels=test_sentiments, \r\n",
        "                                      predicted_labels=svm_bow_predictions,\r\n",
        "                                      classes=['positive', 'negative'])\r\n",
        "\r\n",
        "# THE BELOW O/P SHOULD GIVE YOU A FAIR IDEA ON WHAT :\r\n",
        "# methods like \r\n",
        "# train_predict_model() are doing and printing as o/p.\r\n",
        "# display_model_performance_metrics() are doing and printing as o/p.\r\n",
        "\r\n",
        "# As an Intern you are not suppose to produce the exact o/p \r\n",
        "# You may only code the minimum required metrics which helps you to \r\n",
        "# compare the different ML models."
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Performance metrics:\n",
            "------------------------------\n",
            "Accuracy: 0.8786\n",
            "Precision: 0.8787\n",
            "Recall: 0.8786\n",
            "F1 Score: 0.8785\n",
            "\n",
            "Model Classification report:\n",
            "------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.89      0.87      0.88     12500\n",
            "    negative       0.87      0.89      0.88     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n",
            "\n",
            "Prediction Confusion Matrix:\n",
            "------------------------------\n",
            "                 Predicted:         \n",
            "                   positive negative\n",
            "Actual: positive      10863     1637\n",
            "        negative       1399    11101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLEcwKQhCddM",
        "outputId": "5b858c49-6cca-405f-bd0d-c8995fffdec6"
      },
      "source": [
        "# SVM model on TF-IDF features\r\n",
        "# Please Note : the module meu is not been provided.\r\n",
        "svm_tfidf_predictions = train_predict_model(classifier=svm, \r\n",
        "                                                train_features=tv_train_features, train_labels=train_sentiments,\r\n",
        "                                                test_features=tv_test_features, test_labels=test_sentiments)\r\n",
        "display_model_performance_metrics(true_labels=test_sentiments, \r\n",
        "                                      predicted_labels=svm_tfidf_predictions,\r\n",
        "                                      classes=['positive', 'negative'])\r\n",
        "\r\n",
        "# THE BELOW O/P SHOULD GIVE YOU A FAIR IDEA ON WHAT :\r\n",
        "# methods like \r\n",
        "# train_predict_model() are doing and printing as o/p.\r\n",
        "# display_model_performance_metrics() are doing and printing as o/p.\r\n",
        "\r\n",
        "# As an Intern you are not suppose to produce the exact o/p \r\n",
        "# You may only code the minimum required metrics which helps you to \r\n",
        "# compare the different ML models."
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Performance metrics:\n",
            "------------------------------\n",
            "Accuracy: 0.8927\n",
            "Precision: 0.8928\n",
            "Recall: 0.8927\n",
            "F1 Score: 0.8927\n",
            "\n",
            "Model Classification report:\n",
            "------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.89      0.90      0.89     12500\n",
            "    negative       0.90      0.88      0.89     12500\n",
            "\n",
            "    accuracy                           0.89     25000\n",
            "   macro avg       0.89      0.89      0.89     25000\n",
            "weighted avg       0.89      0.89      0.89     25000\n",
            "\n",
            "\n",
            "Prediction Confusion Matrix:\n",
            "------------------------------\n",
            "                 Predicted:         \n",
            "                   positive negative\n",
            "Actual: positive      11269     1231\n",
            "        negative       1452    11048\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THLsLu73Ob4d",
        "outputId": "cbadd8a3-2af9-482e-dc59-d3d571be0d55"
      },
      "source": [
        "! pip install catboost"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/37/bc4e0ddc30c07a96482abf1de7ed1ca54e59bba2026a33bca6d2ef286e5b/catboost-0.24.4-cp36-none-manylinux1_x86_64.whl (65.7MB)\n",
            "\u001b[K     |████████████████████████████████| 65.8MB 69kB/s \n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.19.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.1.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.24.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQIhEV0VCpAk"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from xgboost import XGBClassifier\r\n",
        "from catboost import CatBoostClassifier\r\n",
        "\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YbynnR6OTOu"
      },
      "source": [
        "randomclassifier=RandomForestClassifier(n_estimators=300,criterion='entropy')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_ye7Sg0OpHl",
        "outputId": "8311825f-6b7a-462d-c009-1a2d47ccee47"
      },
      "source": [
        "# Random Forest model on TF-IDF features\r\n",
        "# Please Note : the module meu is not been provided.\r\n",
        "svm_tfidf_predictions = train_predict_model(classifier=randomclassifier, \r\n",
        "                                                train_features=tv_train_features, train_labels=train_sentiments,\r\n",
        "                                                test_features=tv_test_features, test_labels=test_sentiments)\r\n",
        "display_model_performance_metrics(true_labels=test_sentiments, \r\n",
        "                                      predicted_labels=svm_tfidf_predictions,\r\n",
        "                                      classes=['positive', 'negative'])\r\n",
        "\r\n",
        "# THE BELOW O/P SHOULD GIVE YOU A FAIR IDEA ON WHAT :\r\n",
        "# methods like \r\n",
        "# train_predict_model() are doing and printing as o/p.\r\n",
        "# display_model_performance_metrics() are doing and printing as o/p.\r\n",
        "\r\n",
        "# As an Intern you are not suppose to produce the exact o/p \r\n",
        "# You may only code the minimum required metrics which helps you to \r\n",
        "# compare the different ML models."
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Performance metrics:\n",
            "------------------------------\n",
            "Accuracy: 0.8674\n",
            "Precision: 0.8679\n",
            "Recall: 0.8674\n",
            "F1 Score: 0.8674\n",
            "\n",
            "Model Classification report:\n",
            "------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.88      0.85      0.87     12500\n",
            "    negative       0.86      0.88      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n",
            "\n",
            "Prediction Confusion Matrix:\n",
            "------------------------------\n",
            "                 Predicted:         \n",
            "                   positive negative\n",
            "Actual: positive      10630     1870\n",
            "        negative       1444    11056\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SnzoFlzPESz",
        "outputId": "adfc6ad3-1126-41da-ec45-58dec6821f35"
      },
      "source": [
        "# Randome Forest model on BOW features\r\n",
        "# Please Note : the module meu is not been provided.\r\n",
        "random_bow_predictions = train_predict_model(classifier=randomclassifier, \r\n",
        "                                             train_features=cv_train_features, train_labels=train_sentiments,\r\n",
        "                                             test_features=cv_test_features, test_labels=test_sentiments)\r\n",
        "display_model_performance_metrics(true_labels=test_sentiments, \r\n",
        "                                      predicted_labels=random_bow_predictions,\r\n",
        "                                      classes=['positive', 'negative'])\r\n",
        "\r\n",
        "# THE BELOW O/P SHOULD GIVE YOU A FAIR IDEA ON WHAT :\r\n",
        "# methods like \r\n",
        "# train_predict_model() are doing and printing as o/p.\r\n",
        "# display_model_performance_metrics() are doing and printing as o/p.\r\n",
        "\r\n",
        "# As an Intern you are not suppose to produce the exact o/p \r\n",
        "# You may only code the minimum required metrics which helps you to \r\n",
        "# compare the different ML models."
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Performance metrics:\n",
            "------------------------------\n",
            "Accuracy: 0.8699\n",
            "Precision: 0.87\n",
            "Recall: 0.8699\n",
            "F1 Score: 0.8699\n",
            "\n",
            "Model Classification report:\n",
            "------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.86      0.88      0.87     12500\n",
            "    negative       0.88      0.86      0.87     12500\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.87      0.87      0.87     25000\n",
            "weighted avg       0.87      0.87      0.87     25000\n",
            "\n",
            "\n",
            "Prediction Confusion Matrix:\n",
            "------------------------------\n",
            "                 Predicted:         \n",
            "                   positive negative\n",
            "Actual: positive      10985     1515\n",
            "        negative       1737    10763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FALjdhKEQZss"
      },
      "source": [
        ""
      ],
      "execution_count": 42,
      "outputs": []
    }
  ]
}